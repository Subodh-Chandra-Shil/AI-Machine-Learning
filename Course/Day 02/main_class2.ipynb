{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "from torchvision import transforms,datasets\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset gifted by torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "# To get the Normalization values do the follwing after downloading train data\n",
    "# print(train_data.data.float().mean()/255)\n",
    "# print(train_data.data.float().std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [04:05<00:00, 107428.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 165622.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:08<00:00, 491887.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=datasets.FashionMNIST('data',train=True,download=True,transform=transform)\n",
    "test_data=datasets.FashionMNIST('data',train=False,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size=0.2\n",
    "train_length=len(train_data)\n",
    "\n",
    "indices=[i for i in range(train_length)]\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split=int(np.floor(valid_size*train_length))\n",
    "train_idx=indices[split:]\n",
    "valid_idx=indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size : 48000, Validation data size : 12000, Test data size : 10000\n"
     ]
    }
   ],
   "source": [
    "num_workers=0\n",
    "batch_size=20\n",
    "train_loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,num_workers=num_workers)\n",
    "valid_loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,num_workers=num_workers)\n",
    "test_loader=torch.utils.data.DataLoader(test_data,batch_size=batch_size,num_workers=num_workers)\n",
    "\n",
    "# This is for debugging\n",
    "print(f\"Training data size : {train_idx.__len__()}, Validation data size : {valid_idx.__len__()}, Test data size : {test_loader.dataset.__len__()}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will do a single project. We will create a dataset for image denoising. We will create training, validation and test dataset with by adding some noises on the clean images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy2\n",
    "import os \n",
    "import cv2\n",
    "import glob\n",
    "import platform\n",
    "delimeter = \"\\\\\" if platform.system() == \"Windows\" else \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(data_dir, target_dir):\n",
    "    target = os.path.join(data_dir, target_dir)\n",
    "    if not os.path.exists(target):\n",
    "        os.mkdir(target)\n",
    "    return target\n",
    "                                    \n",
    "def writefile(target_dir, filename, data):\n",
    "    filepath = os.path.join(target_dir, filename)\n",
    "    cv2.imwrite(filepath, data)\n",
    "    \n",
    "                                                 \n",
    "def copyfile(source_dir, dest_dir):\n",
    "    sources = glob.glob(os.path.join(source_dir,'*'))\n",
    "    for i in sources:\n",
    "        filename = i.split('\\\\')[-1]\n",
    "        filepath = os.path.join(dest_dir, filename)\n",
    "        copy2(i, filepath)\n",
    "\n",
    "def add_noise(input_img,sigma):\n",
    "    noise = np.random.normal(scale=sigma/255, size=input_img.shape)\n",
    "    add_noise = np.clip(input_img + noise, 0, 1)\n",
    "    return np.float32(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(dataset_train_dir, dataset_val_dir, save_dir):\n",
    "    clean_train = glob.glob(os.path.join(dataset_train_dir,'*'))\n",
    "    clean_train.sort()\n",
    "    \n",
    "    noisy_input_dir = create_folder(os.path.join(save_dir,\"train_trial\"),\"input\")\n",
    "    clean_target_dir = create_folder(os.path.join(save_dir,\"train_trial\"), \"gt\")\n",
    "    copyfile(dataset_train_dir, clean_target_dir)\n",
    "    for i in clean_train:\n",
    "        im_path = i\n",
    "        im_name = im_path.split(delimeter)[-1]\n",
    "        img = cv2.imread(i)\n",
    "        y1 = np.uint8(255*add_noise(img/255, sigma=50))\n",
    "        cv2.imwrite(os.path.join(noisy_input_dir,im_name), y1)\n",
    "    print(\"training noisy data prepared\")\n",
    "    \n",
    "    clean_val = glob.glob(os.path.join(dataset_val_dir,'*'))\n",
    "    clean_val.sort()\n",
    "    noisy_val_dir = create_folder(os.path.join(save_dir,\"val_trial\"), \"noisy\")\n",
    "    clean_val_dir = create_folder(os.path.join(save_dir,\"val_trial\"), \"clean\")\n",
    "    copyfile(dataset_val_dir, clean_val_dir)\n",
    "    for k in clean_val:\n",
    "        val_path = k\n",
    "        val_name = val_path.split(delimeter)[-1]\n",
    "        img_val = cv2.imread(k)\n",
    "        y_val = np.uint8(255*add_noise(img/255, sigma=50))\n",
    "        target_dir_val = os.path.join(noisy_val_dir, val_name)\n",
    "        cv2.imwrite(target_dir_val, y_val)\n",
    "    print(\"validation noisy data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preparing training and validation data................!!!!\")\n",
    "dataset_train_dir = 'Give your path here' \n",
    "dataset_val_dir = 'Give your path here'\n",
    "save_dir = 'Give your path here'\n",
    "prepare_training_dataset(dataset_train_dir, dataset_val_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testdataset(data_dir, dataset_name):\n",
    "    files = glob.glob(os.path.join(data_dir, dataset_name, '*.png'))\n",
    "    files.sort()\n",
    "    noisy_dir = create_folder(data_dir, dataset_name + \"_noisy\")\n",
    "    for i in range(len(files)):\n",
    "        im_path = i\n",
    "        im_name = im_path.split('\\\\\\\\')[-1]\n",
    "        img = cv2.imread(files[i])\n",
    "        test = np.uint8(255*add_noise(img/255, sigma=50))\n",
    "        cv2.imwrite(os.path.join(noisy_dir,im_name), test)\n",
    "    print(\"test noisy data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'your directory is here'\n",
    "dataset_name = 'Give your dataset name'\n",
    "prepare_testdataset(data_dir, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
